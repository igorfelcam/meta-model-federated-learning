{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, optimizers, utils, losses\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CnnModel Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "```\n",
    "```\n",
    "---\n",
    "Methods\n",
    "```\n",
    "build(): self\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnModel:\n",
    "    @staticmethod\n",
    "\n",
    "    def build():\n",
    "        model = models.Sequential()\n",
    "\n",
    "        model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(32, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "\n",
    "        model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "        model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Dropout(0.5))\n",
    "\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(128, activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=losses.categorical_crossentropy,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestModel Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "```\n",
    "```\n",
    "---\n",
    "Methods\n",
    "```\n",
    "build(): self\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestModel:\n",
    "    @staticmethod\n",
    "\n",
    "    def build():\n",
    "        return RandomForestClassifier(\n",
    "            criterion='gini',\n",
    "            max_depth=10,\n",
    "            max_features='sqrt',\n",
    "            min_samples_leaf=10\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNearestNeighborsModelClass\n",
    "\n",
    "---\n",
    "Atributes\n",
    "```\n",
    "```\n",
    "---\n",
    "Methods\n",
    "```\n",
    "build(): self\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighborsModel:\n",
    "    @staticmethod\n",
    "\n",
    "    def build():\n",
    "        return KNeighborsClassifier(\n",
    "            n_neighbors=3,\n",
    "            weights='distance',\n",
    "            algorithm='brute'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBatch Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "\n",
    "```\n",
    "_test_images: dict = {}\n",
    "_test_labels: dict = {}\n",
    "_train_images: dict = {}\n",
    "_train_labels: dict = {}\n",
    "_shards: dict = {}\n",
    "_num_classes = None\n",
    "_num_clients = None\n",
    "```\n",
    "\n",
    "---\n",
    "Methods\n",
    "```\n",
    "getters(): self\n",
    "_split_data(id_type): void\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBatch:\n",
    "\n",
    "    # attributes\n",
    "    _test_images: dict = {}\n",
    "    _test_labels: dict = {}\n",
    "    _train_images: dict = {}\n",
    "    _train_labels: dict = {}\n",
    "    _shards: dict = {}\n",
    "    _num_classes = None\n",
    "    _num_clients = None\n",
    "\n",
    "\n",
    "    def __init__(self, num_clients, num_classes):\n",
    "        self._num_clients = num_clients\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "\n",
    "    # getters\n",
    "    def get_test_images(self, id_type):\n",
    "        return self._test_images[id_type]\n",
    "\n",
    "\n",
    "    def get_test_labels(self, id_type):\n",
    "        return self._test_labels[id_type]\n",
    "\n",
    "\n",
    "    def get_train_images(self, id_type):\n",
    "        return self._train_images[id_type]\n",
    "\n",
    "\n",
    "    def get_train_labels(self, id_type):\n",
    "        return self._train_labels[id_type]\n",
    "\n",
    "\n",
    "    def get_shards(self, id_type):\n",
    "        return self._shards[id_type]\n",
    "\n",
    "\n",
    "    def get_full_shards(self):\n",
    "        return self._shards\n",
    "\n",
    "\n",
    "    # methods\n",
    "    def _split_data_iid(self, id_type):\n",
    "        data = list(zip(\n",
    "            self._train_images[id_type],\n",
    "            self._train_labels[id_type]\n",
    "        ))\n",
    "        random.shuffle(data)\n",
    "        train, label = zip(*data)\n",
    "\n",
    "        len_shards = len(self._train_images[id_type]) // self._num_clients\n",
    "        range_start_shard = 0\n",
    "        range_final_shard = len_shards\n",
    "\n",
    "        shards = {}\n",
    "        i = 0\n",
    "        for i in range(self._num_clients):\n",
    "            shards[i] = {\n",
    "                'train_images': [],\n",
    "                'train_labels': []\n",
    "            }\n",
    "\n",
    "            shards[i]['train_images'] = train[range_start_shard:range_final_shard]\n",
    "            shards[i]['train_labels'] = label[range_start_shard:range_final_shard]\n",
    "\n",
    "            range_start_shard = range_final_shard\n",
    "            range_final_shard = range_final_shard + len_shards\n",
    "\n",
    "        self._shards[id_type] = shards\n",
    "    \n",
    "    \n",
    "    def _split_data_non_iid(self, id_type):\n",
    "        data = list(zip(\n",
    "            self._train_images[id_type],\n",
    "            self._train_labels[id_type]\n",
    "        ))\n",
    "        \n",
    "        if id_type == 'cnn':\n",
    "            data.sort(key=lambda e: np.argmax(e[1]))\n",
    "        else:\n",
    "            data.sort(key=lambda e: e[1])\n",
    "        \n",
    "        x_order_train, y_order_train = zip(*data)\n",
    "        \n",
    "        shards = {}\n",
    "        i = 0\n",
    "        for i in range(self._num_clients):\n",
    "            j = 0\n",
    "            shards[i] = {\n",
    "                'train_images': [],\n",
    "                'train_labels': [],\n",
    "                'data_count': {j: 0 for j in range(self._num_classes)}\n",
    "            }\n",
    "        \n",
    "        label_proportion = (.38, .12, .12, .09, .09, .05, .05, .04, .03, .03)\n",
    "\n",
    "        assert self._num_clients == len(label_proportion)\n",
    "        assert sum(label_proportion) == 1\n",
    "\n",
    "        len_label_sample = len(x_order_train) // self._num_clients\n",
    "\n",
    "        c = start = end = 0\n",
    "        for c in range(self._num_clients):\n",
    "            key_shard = c\n",
    "\n",
    "            i = p = 0\n",
    "            for i, p in enumerate(label_proportion):\n",
    "                start = end\n",
    "                end = start + int(len_label_sample * p)\n",
    "\n",
    "                if key_shard == self._num_clients:\n",
    "                    key_shard = 0\n",
    "\n",
    "                shards[key_shard]['train_images'] += x_order_train[start:end]\n",
    "                shards[key_shard]['train_labels'] += y_order_train[start:end]\n",
    "\n",
    "                key_shard += 1\n",
    "\n",
    "        self._shards[id_type] = shards\n",
    "        \n",
    "        \n",
    "    def _show_distribution(self, id_type):\n",
    "        i = 0\n",
    "        for i in range(self._num_clients):\n",
    "\n",
    "            j = y = 0\n",
    "            for j, y in enumerate(self._shards[id_type][i]['train_labels']):\n",
    "                if id_type == 'cnn':\n",
    "                    self._shards[id_type][i]['data_count'][(np.argmax(y))] += 1\n",
    "                else:\n",
    "                    self._shards[id_type][i]['data_count'][y] += 1\n",
    "\n",
    "            print(\n",
    "                len(self._shards[id_type][i]['train_images']),\n",
    "                len(self._shards[id_type][i]['train_labels']),\n",
    "                self._shards[id_type][i]['data_count']\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CnnData(DataBatch) Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "\n",
    "```\n",
    "__TYPE = 'cnn'\n",
    "```\n",
    "\n",
    "---\n",
    "Methods\n",
    "```\n",
    "__prepare_data_to_cnn: void\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnData(DataBatch):\n",
    "\n",
    "    # attributes\n",
    "    __TYPE = 'cnn'\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, num_clients, num_classes):\n",
    "        super().__init__(num_clients, num_classes)\n",
    "        (self._train_images[self.__TYPE], self._train_labels[self.__TYPE]), (self._test_images[self.__TYPE], self._test_labels[self.__TYPE]) = dataset\n",
    "        self.__prepare_data_to_cnn()\n",
    "        self._split_data_non_iid(self.__TYPE)\n",
    "        self._show_distribution(self.__TYPE)\n",
    "\n",
    "\n",
    "    # methods\n",
    "    def __prepare_data_to_cnn(self):\n",
    "        self._train_labels[self.__TYPE] = utils.to_categorical(\n",
    "            self._train_labels[self.__TYPE],\n",
    "            self._num_classes\n",
    "        )\n",
    "\n",
    "        self._test_labels[self.__TYPE] = utils.to_categorical(\n",
    "            self._test_labels[self.__TYPE],\n",
    "            self._num_classes\n",
    "        )\n",
    "\n",
    "        self._train_images[self.__TYPE] = self._train_images[self.__TYPE].astype('float32')\n",
    "        self._test_images[self.__TYPE] = self._test_images[self.__TYPE].astype('float32')\n",
    "\n",
    "        self._train_images[self.__TYPE] /= 255.\n",
    "        self._test_images[self.__TYPE] /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestData(DataBatch) Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "\n",
    "```\n",
    "__TYPE = 'rf'\n",
    "```\n",
    "\n",
    "---\n",
    "Methods\n",
    "```\n",
    "__prepare_data_to_rf: void\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestData(DataBatch):\n",
    "\n",
    "    # attributes\n",
    "    __TYPE = 'rf'\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, num_clients, num_classes):\n",
    "        super().__init__(num_clients, num_classes)\n",
    "        (self._train_images[self.__TYPE], self._train_labels[self.__TYPE]), (self._test_images[self.__TYPE], self._test_labels[self.__TYPE]) = dataset\n",
    "        self.__prepare_data_to_rf()\n",
    "        self._split_data_non_iid(self.__TYPE)\n",
    "        self._show_distribution(self.__TYPE)\n",
    "\n",
    "\n",
    "    # methods\n",
    "    def __prepare_data_to_rf(self):\n",
    "        self._train_labels[self.__TYPE] = self._train_labels[self.__TYPE].flatten()\n",
    "        self._test_labels[self.__TYPE] = self._test_labels[self.__TYPE].flatten()\n",
    "\n",
    "        self._train_images[self.__TYPE] = self._train_images[self.__TYPE].reshape(50000, 3*32*32)\n",
    "        self._test_images[self.__TYPE] = self._test_images[self.__TYPE].reshape(10000, 3*32*32)\n",
    "\n",
    "        self._train_images[self.__TYPE] = self._train_images[self.__TYPE].astype('float32')\n",
    "        self._test_images[self.__TYPE] = self._test_images[self.__TYPE].astype('float32')\n",
    "\n",
    "        self._train_images[self.__TYPE] /= 255.\n",
    "        self._test_images[self.__TYPE] /= 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNearestNeighborsData(DataBatch) Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "\n",
    "```\n",
    "__TYPE = 'knn'\n",
    "```\n",
    "\n",
    "---\n",
    "Methods\n",
    "```\n",
    "__prepare_data_to_knn: void\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighborsData(DataBatch):\n",
    "\n",
    "    # attributes\n",
    "    __TYPE = 'knn'\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, num_clients, num_classes):\n",
    "        super().__init__(num_clients, num_classes)\n",
    "        (self._train_images[self.__TYPE], self._train_labels[self.__TYPE]), (self._test_images[self.__TYPE], self._test_labels[self.__TYPE]) = dataset\n",
    "        self.__prepare_data_to_knn()\n",
    "        self._split_data_non_iid(self.__TYPE)\n",
    "        self._show_distribution(self.__TYPE)\n",
    "\n",
    "\n",
    "    # methods\n",
    "    def __prepare_data_to_knn(self):\n",
    "        self._train_labels[self.__TYPE] = self._train_labels[self.__TYPE].flatten()\n",
    "        self._test_labels[self.__TYPE] = self._test_labels[self.__TYPE].flatten()\n",
    "\n",
    "        self._train_images[self.__TYPE] = self._train_images[self.__TYPE]/255.0\n",
    "        self._test_images[self.__TYPE] = self._test_images[self.__TYPE]/255.0\n",
    "\n",
    "        n_samples, nx, ny, nrgb = self._train_images[self.__TYPE].shape\n",
    "        self._train_images[self.__TYPE] = self._train_images[self.__TYPE].reshape((n_samples, nx*ny*nrgb)) # (50000, 32*32*3)\n",
    "\n",
    "        n_samples, nx, ny, nrgb = self._test_images[self.__TYPE].shape\n",
    "        self._test_images[self.__TYPE] = self._test_images[self.__TYPE].reshape((n_samples, nx*ny*nrgb)) # (10000, 32*32*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClientHost Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "```\n",
    "name: str\n",
    "data\n",
    "model\n",
    "prediction\n",
    "history\n",
    "```\n",
    "---\n",
    "Methods\n",
    "```\n",
    "getters(): self\n",
    "setters(self): void\n",
    "fit_model(id_type, test_images, test_labels): void\n",
    "plot_accuracy(id_type): void\n",
    "plot_loss(id_type): void\n",
    "print_details_train(): void\n",
    "make_predict(id_type, test_sample): void\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientHost:\n",
    "    \n",
    "    # attributes\n",
    "    __name: str = ''\n",
    "    __data_batches: dict = {}\n",
    "    __models: dict = {}\n",
    "    __histories: dict = {}\n",
    "    __predictions: dict = {}\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__name = ''.join(\n",
    "            random.choice(\n",
    "                string.ascii_uppercase + string.digits\n",
    "            ) for _ in range(6)\n",
    "        )\n",
    "\n",
    "\n",
    "    # getters\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.__name\n",
    "\n",
    "    @property\n",
    "    def data_batches(self):\n",
    "        return self.__data_batches\n",
    "\n",
    "    @property\n",
    "    def models(self):\n",
    "        return self.__models\n",
    "\n",
    "    @property\n",
    "    def histories(self):\n",
    "        return self.__histories\n",
    "    \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        return self.__predictions\n",
    "\n",
    "\n",
    "    # setters\n",
    "    @data_batches.setter\n",
    "    def data_batches(self, data_batches):\n",
    "        self.__data_batches = data_batches\n",
    "\n",
    "    @models.setter\n",
    "    def models(self, models):\n",
    "        self.__models = models\n",
    "    \n",
    "    @histories.setter\n",
    "    def histories(self, histories):\n",
    "        self.__histories = histories\n",
    "\n",
    "    @predictions.setter\n",
    "    def predictions(self, predictions):\n",
    "        self.__predictions = predictions\n",
    "\n",
    "        \n",
    "    # class methods\n",
    "    @staticmethod\n",
    "    def fit_model(id_type, model, train_images, train_labels, test_images=None, test_labels=None, epochs=None):\n",
    "        data_batch = {}\n",
    "        \n",
    "        if id_type == 'cnn':\n",
    "            history = model.fit(\n",
    "                x=tf.stack(train_images),\n",
    "                y=tf.stack(train_labels),\n",
    "                epochs=epochs,\n",
    "                validation_data=(\n",
    "                    test_images,\n",
    "                    test_labels\n",
    "                )\n",
    "            )\n",
    "            print(id_type,\n",
    "                  np.argmax(train_labels[0]),\n",
    "                  np.argmax(train_labels[2500]),\n",
    "                  np.argmax(train_labels[4999])\n",
    "            )\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                X=train_images,\n",
    "                y=train_labels\n",
    "            )\n",
    "            print(id_type, train_labels[0], train_labels[2500], train_labels[4999])\n",
    "            \n",
    "        data_batch = {\n",
    "            'train_images': train_images,\n",
    "            'train_labels': train_labels\n",
    "        }\n",
    "        \n",
    "        return history, model, data_batch\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def make_predict(id_type, model, test_sample):\n",
    "        predictions = []\n",
    "\n",
    "        for sample in test_sample:\n",
    "            image = np.expand_dims(sample['image'], 0)\n",
    "            prediction = model.predict(image)\n",
    "            \n",
    "            if id_type == 'cnn':\n",
    "                predict = np.argmax(prediction[0])\n",
    "                full_predict = prediction[0]\n",
    "            else:\n",
    "                predict = prediction[0]\n",
    "                full_predict = model.predict_proba(image)\n",
    "\n",
    "            predictions.append({\n",
    "                'label': sample['label'],\n",
    "                'predict': predict,\n",
    "                'full_predict': full_predict\n",
    "            })\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    # def plot_accuracy(self, id_type):\n",
    "    #   plt.figure(figsize=[6,4])\n",
    "    #   plt.plot(self.__histories[id_type].history['accuracy'], 'black', linewidth=2.0)\n",
    "    #   plt.plot(self.__histories[id_type].history['val_accuracy'], 'blue', linewidth=2.0)\n",
    "    #   plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\n",
    "    #   plt.xlabel('Epochs', fontsize=10)\n",
    "    #   plt.ylabel('Accuracy', fontsize=10)\n",
    "    #   plt.title('Accuracy Curves', fontsize=12)\n",
    "    #   plt.show()\n",
    "\n",
    "\n",
    "    # def plot_loss(self, id_type):\n",
    "    #   plt.figure(figsize=[6,4])\n",
    "    #   plt.plot(self.__histories[id_type].history['loss'], 'black', linewidth=2.0)\n",
    "    #   plt.plot(self.__histories[id_type].history['val_loss'], 'green', linewidth=2.0)\n",
    "    #   plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\n",
    "    #   plt.xlabel('Epochs', fontsize=10)\n",
    "    #   plt.ylabel('Loss', fontsize=10)\n",
    "    #   plt.title('Loss Curves', fontsize=12)\n",
    "    #   plt.show()\n",
    "\n",
    "\n",
    "    # def print_details_train(self):\n",
    "    #   test_loss, test_acc = self.__models.evaluate(\n",
    "    #     self._test_images,\n",
    "    #     self._test_labels,\n",
    "    #     verbose=2\n",
    "    #   )\n",
    "\n",
    "    #   print(\n",
    "    #     'Client: {} | accuracy: {:.2%} | loss: {} \\n\\n\\n'.format(\n",
    "    #       self._name,\n",
    "    #       test_acc,\n",
    "    #       test_loss\n",
    "    #     )\n",
    "    #   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConsensusMechanisms Class\n",
    "\n",
    "---\n",
    "Atributes\n",
    "```\n",
    "```\n",
    "---\n",
    "Methods\n",
    "```\n",
    "## public ##\n",
    "plotHitRate(): void\n",
    "makeMajority(): void\n",
    "makeWeightedMajority(): void\n",
    "makeBordaCount(): void\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsensusMechanisms:\n",
    "    \n",
    "    # class methods\n",
    "    def __is_unanimous(self, voting_list):\n",
    "        count = 0\n",
    "        for key, value in voting_list.items():\n",
    "            if value > 0:\n",
    "                count = count + 1\n",
    "\n",
    "        return count == 1\n",
    "    \n",
    "    \n",
    "    def plot_hit_rate(self, voting):\n",
    "        hits = 0\n",
    "\n",
    "        for vote in voting:\n",
    "            if vote['correct']:\n",
    "                hits += 1\n",
    "\n",
    "        if hits > 0:\n",
    "            return 'Hit rate {:.4%}'.format((hits / len(voting)))\n",
    "        else:\n",
    "            return 'Hit rate 0%'\n",
    "\n",
    "\n",
    "    def make_majority(self, type_sample, test_sample, client_list):\n",
    "        majority_voting = list()\n",
    "\n",
    "        for label_id, sample in enumerate(test_sample):\n",
    "            \n",
    "            if type(sample['label']) is np.ndarray:\n",
    "                label_value = np.argmax(sample['label'])\n",
    "            else:\n",
    "                label_value = sample['label']\n",
    "\n",
    "            i = 0\n",
    "            voting = {i: 0 for i in range(10)}\n",
    "\n",
    "            correct = False\n",
    "            highest_value = 0\n",
    "\n",
    "            for client in client_list:\n",
    "                client_prediction = client.predictions[type_sample][label_id]\n",
    "                voting[client_prediction['predict']] += 1\n",
    "\n",
    "            highest_value = max(voting, key=voting.get)\n",
    "            \n",
    "            if label_value == highest_value:\n",
    "                correct = True\n",
    "\n",
    "            unanimous = self.__is_unanimous(voting)\n",
    "\n",
    "            majority_voting.append(\n",
    "                {\n",
    "                    'target': label_value,\n",
    "                    'correct': correct,\n",
    "                    'unanimous': unanimous,\n",
    "                    'voting': voting\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return majority_voting\n",
    "\n",
    "  \n",
    "    def make_weighted_majority(self, type_sample, test_sample, client_list):\n",
    "        weighted_majority_voting = list()\n",
    "        len_clients = len(client_list)\n",
    "\n",
    "        for label_id, sample in enumerate(test_sample):\n",
    "            \n",
    "            if type(sample['label']) is np.ndarray:\n",
    "                label_value = np.argmax(sample['label'])\n",
    "            else:\n",
    "                label_value = sample['label']\n",
    "\n",
    "            i = 0\n",
    "            voting = {i: 0 for i in range(10)}\n",
    "            correct = False\n",
    "\n",
    "            for client in client_list:\n",
    "                client_prediction = client.predictions[type_sample][label_id]\n",
    "                npArgmax = client_prediction['predict']\n",
    "\n",
    "                if len(client_prediction['full_predict']) == 1:\n",
    "                    certainty_level = client_prediction['full_predict'][0][npArgmax]\n",
    "                else:\n",
    "                    certainty_level = client_prediction['full_predict'][npArgmax]\n",
    "\n",
    "                certainty_coefficient = certainty_level + 1\n",
    "                voting[npArgmax] += (certainty_coefficient * certainty_level)\n",
    "\n",
    "            if label_value == max(voting, key=voting.get):\n",
    "                correct = True\n",
    "            \n",
    "            unanimous = self.__is_unanimous(voting)\n",
    "\n",
    "            weighted_majority_voting.append(\n",
    "                {\n",
    "                    'target': label_value,\n",
    "                    'correct': correct,\n",
    "                    'unanimous': unanimous,\n",
    "                    'voting': voting\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return weighted_majority_voting\n",
    "\n",
    "  \n",
    "    def make_borda_count(self, type_sample, test_sample, client_list):\n",
    "        borda_count_voting = list()\n",
    "        len_clients = len(client_list)\n",
    "        clients_ballots = list()\n",
    "\n",
    "        for client in client_list:\n",
    "            \n",
    "            client_ballot = {\n",
    "                'client': client.name,\n",
    "                'ballots': list()\n",
    "            }\n",
    "\n",
    "            for predictions in client.predictions[type_sample]:\n",
    "                ballot = list()\n",
    "                \n",
    "                if len(predictions['full_predict']) == 1:\n",
    "                    full_predict = predictions['full_predict'][0]\n",
    "                else:\n",
    "                    full_predict = predictions['full_predict']\n",
    "                    \n",
    "                for option, confidence in enumerate(full_predict):\n",
    "                    ballot.append({\n",
    "                        'option': option,\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "\n",
    "                ballot.sort(reverse=True, key=lambda e: e['confidence'])\n",
    "                max_weight = len(ballot)\n",
    "\n",
    "                for coefficient, vote in enumerate(ballot):\n",
    "                    vote['weight'] = max_weight - coefficient\n",
    "\n",
    "                client_ballot['ballots'].append(ballot)\n",
    "\n",
    "            clients_ballots.append(client_ballot)\n",
    "\n",
    "\n",
    "        for label_id, sample in enumerate(test_sample):\n",
    "            \n",
    "            if type(sample['label']) is np.ndarray:\n",
    "                label_value = np.argmax(sample['label'])\n",
    "            else:\n",
    "                label_value = sample['label']\n",
    "\n",
    "            i = 0\n",
    "            voting = {i: 0 for i in range(10)}\n",
    "            correct = False\n",
    "\n",
    "            for client in clients_ballots:\n",
    "                for item_ballot in client['ballots'][label_id]:\n",
    "                    voting[item_ballot['option']] += item_ballot['weight']\n",
    "\n",
    "            if label_value == max(voting, key=voting.get):\n",
    "                correct = True\n",
    "            \n",
    "            unanimous = self.__is_unanimous(voting)\n",
    "\n",
    "            borda_count_voting.append({\n",
    "                'target': label_value,\n",
    "                'correct': correct,\n",
    "                'unanimous': unanimous,\n",
    "                'voting': voting\n",
    "            })\n",
    "\n",
    "        return borda_count_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application excution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepapre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_RANGE = 10\n",
    "CLASSES_RANGE = 10\n",
    "loaded_data = datasets.cifar10.load_data()\n",
    "\n",
    "\n",
    "print('CNN')\n",
    "dataset = CnnData(loaded_data, CLIENT_RANGE, CLASSES_RANGE)\n",
    "print('RF')\n",
    "dataset = RandomForestData(loaded_data, CLIENT_RANGE, CLASSES_RANGE)\n",
    "print('KNN')\n",
    "dataset = KNearestNeighborsData(loaded_data, CLIENT_RANGE, CLASSES_RANGE)\n",
    "\n",
    "\n",
    "print('CNN dataset test len:', len(dataset.get_test_images('cnn')))        # 10.000\n",
    "print('RF dataset test len:', len(dataset.get_test_images('rf')))          # 10.000\n",
    "print('KNN dataset test len:', len(dataset.get_test_images('knn')))        # 10.000\n",
    "\n",
    "print('CNN dataset train len:', len(dataset.get_train_images('cnn')))      # 50.000\n",
    "print('RF dataset train len:', len(dataset.get_train_images('rf')))        # 50.000\n",
    "print('KNN dataset train len:', len(dataset.get_train_images('knn')))      # 50.000\n",
    "\n",
    "print('CNN total clients:', len(dataset.get_shards('cnn')))\n",
    "print('RF total clients:', len(dataset.get_shards('rf')))\n",
    "print('KNN total clients:', len(dataset.get_shards('knn')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sample CNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_IMAGES = dataset.get_test_images('cnn')\n",
    "CNN_LABELS = dataset.get_test_labels('cnn')\n",
    "\n",
    "cnn_test_sample = list()\n",
    "\n",
    "# SAMPLE_LENGTH = 100   # 100\n",
    "# SAMPLE_LENGTH = int(len(CNN_IMAGES) * 0.1)   # 10%\n",
    "SAMPLE_LENGTH = len(CNN_IMAGES)   # 100%\n",
    "\n",
    "for i in range(SAMPLE_LENGTH):\n",
    "    cnn_test_sample.append({\n",
    "        'image': CNN_IMAGES[i],\n",
    "        'label': CNN_LABELS[i]\n",
    "    })\n",
    "\n",
    "print(SAMPLE_LENGTH, len(cnn_test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sample RF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_IMAGES = dataset.get_test_images('rf')\n",
    "RF_LABELS = dataset.get_test_labels('rf')\n",
    "\n",
    "rf_test_sample = list()\n",
    "\n",
    "# SAMPLE_LENGTH = 100   # 100\n",
    "# SAMPLE_LENGTH = int(len(RF_IMAGES) * 0.1)   # 10%\n",
    "SAMPLE_LENGTH = len(RF_IMAGES)   # 100%\n",
    "\n",
    "for i in range(SAMPLE_LENGTH):\n",
    "    rf_test_sample.append({\n",
    "        'image': RF_IMAGES[i],\n",
    "        'label': RF_LABELS[i]\n",
    "    })\n",
    "\n",
    "print(SAMPLE_LENGTH, len(rf_test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sample KNN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_IMAGES = dataset.get_test_images('knn')\n",
    "KNN_LABELS = dataset.get_test_labels('knn')\n",
    "\n",
    "knn_test_sample = list()\n",
    "\n",
    "# SAMPLE_LENGTH = 100   # 100\n",
    "# SAMPLE_LENGTH = int(len(KNN_IMAGES) * 0.1)   # 10%\n",
    "SAMPLE_LENGTH = len(KNN_IMAGES)   # 100%\n",
    "\n",
    "for i in range(SAMPLE_LENGTH):\n",
    "    knn_test_sample.append({\n",
    "        'image': KNN_IMAGES[i],\n",
    "        'label': KNN_LABELS[i]\n",
    "    })\n",
    "\n",
    "print(SAMPLE_LENGTH, len(knn_test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare clients (federate nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client_list = []\n",
    "\n",
    "i = 0\n",
    "for i in range(CLIENT_RANGE):\n",
    "    client_list.append(ClientHost())\n",
    "\n",
    "\n",
    "i = 0\n",
    "for i, client in enumerate(client_list):\n",
    "    \n",
    "    histories = {}\n",
    "    client_models = {}\n",
    "    data_batches = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    print(i, '-', client.name)\n",
    "\n",
    "    print('To CNN')\n",
    "    \n",
    "    history, model, data_batch = client.fit_model(\n",
    "        id_type='cnn',\n",
    "        model=CnnModel().build(),\n",
    "        train_images=dataset.get_shards('cnn')[i]['train_images'],\n",
    "        train_labels=dataset.get_shards('cnn')[i]['train_labels'],\n",
    "        test_images=dataset.get_test_images('cnn'),\n",
    "        test_labels=dataset.get_test_labels('cnn'),\n",
    "        epochs=100\n",
    "    )\n",
    "    \n",
    "    predict = client.make_predict(\n",
    "        id_type='cnn',\n",
    "        model=model,\n",
    "        test_sample=cnn_test_sample\n",
    "    )\n",
    "    \n",
    "    histories['cnn'] = history\n",
    "    client_models['cnn'] = model\n",
    "    data_batches['cnn'] = data_batch\n",
    "    predictions['cnn'] = predict\n",
    "    \n",
    "    # client.plot_accuracy('cnn')\n",
    "    # client.plot_loss('cnn')\n",
    "    print('=====\\n')\n",
    "\n",
    "\n",
    "    print('To Random Forest')\n",
    "    \n",
    "    history, model, data_batch = client.fit_model(\n",
    "        id_type='rf',\n",
    "        model=RandomForestModel().build(),\n",
    "        train_images=dataset.get_shards('rf')[i]['train_images'],\n",
    "        train_labels=dataset.get_shards('rf')[i]['train_labels']\n",
    "    )\n",
    "    \n",
    "    predict = client.make_predict(\n",
    "        id_type='rf',\n",
    "        model=model,\n",
    "        test_sample=rf_test_sample\n",
    "    )\n",
    "    \n",
    "    histories['rf'] = history\n",
    "    client_models['rf'] = model\n",
    "    data_batches['rf'] = data_batch\n",
    "    predictions['rf'] = predict\n",
    "    \n",
    "    # client.plot_accuracy('rf')\n",
    "    # client.plot_loss('rf')\n",
    "    print('=====\\n')\n",
    "    \n",
    "    \n",
    "    print('To K-Nearest Neighbors')\n",
    "    \n",
    "    history, model, data_batch = client.fit_model(\n",
    "        id_type='knn',\n",
    "        model=KNearestNeighborsModel().build(),\n",
    "        train_images=dataset.get_shards('knn')[i]['train_images'],\n",
    "        train_labels=dataset.get_shards('knn')[i]['train_labels']\n",
    "    )\n",
    "    \n",
    "    predict = client.make_predict(\n",
    "        id_type='knn',\n",
    "        model=model,\n",
    "        test_sample=knn_test_sample\n",
    "    )\n",
    "    \n",
    "    histories['knn'] = history\n",
    "    client_models['knn'] = model\n",
    "    data_batches['knn'] = data_batch\n",
    "    predictions['knn'] = predict\n",
    "    \n",
    "    # client.plot_accuracy('knn')\n",
    "    # client.plot_loss('knn')\n",
    "    \n",
    "    print('=====\\n')\n",
    "    \n",
    "\n",
    "    client.histories = histories\n",
    "    client.models = client_models\n",
    "    client.data_batches = data_batches\n",
    "    client.predictions = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "consensus_mechanisms = ConsensusMechanisms()\n",
    "\n",
    "types_sample = [\n",
    "    'cnn',\n",
    "    'rf',\n",
    "    'knn'\n",
    "]\n",
    "\n",
    "majority_voting = {}\n",
    "weighted_majority_voting = {}\n",
    "borda_count_voting = {}\n",
    "\n",
    "for type_sample in types_sample:\n",
    "    print('To', type_sample)\n",
    "\n",
    "    # --\n",
    "    majority_voting[type_sample] = consensus_mechanisms.make_majority(\n",
    "        type_sample,\n",
    "        globals().get(type_sample + '%s' % '_test_sample'),\n",
    "        client_list\n",
    "    )\n",
    "    print('Majority', consensus_mechanisms.plot_hit_rate(majority_voting[type_sample]))\n",
    "\n",
    "    # --\n",
    "    weighted_majority_voting[type_sample] = consensus_mechanisms.make_weighted_majority(\n",
    "        type_sample,\n",
    "        globals().get(type_sample + '%s' % '_test_sample'),\n",
    "        client_list\n",
    "    )\n",
    "    print('Weighted Majority', consensus_mechanisms.plot_hit_rate(weighted_majority_voting[type_sample]))\n",
    "\n",
    "    # --\n",
    "    borda_count_voting[type_sample] = consensus_mechanisms.make_borda_count(\n",
    "        type_sample,\n",
    "        globals().get(type_sample + '%s' % '_test_sample'),\n",
    "        client_list\n",
    "    )\n",
    "    print('Borda Count', consensus_mechanisms.plot_hit_rate(borda_count_voting[type_sample]))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "for i in random.sample(range(100), k=5):\n",
    "    print('Index sample', i)\n",
    "    print('\\n')\n",
    "    \n",
    "    print('To cnn')\n",
    "    print('Majority', majority_voting['cnn'][i])\n",
    "    print('Weighted Majority', weighted_majority_voting['cnn'][i])\n",
    "    print('Borda Count', borda_count_voting['cnn'][i])\n",
    "    print('\\n')\n",
    "\n",
    "    print('To rf')\n",
    "    print('Majority', majority_voting['rf'][i])\n",
    "    print('Weighted Majority', weighted_majority_voting['rf'][i])\n",
    "    print('Borda Count', borda_count_voting['rf'][i])\n",
    "    print('\\n')\n",
    "    \n",
    "    print('To knn')\n",
    "    print('Majority', majority_voting['knn'][i])\n",
    "    print('Weighted Majority', weighted_majority_voting['knn'][i])\n",
    "    print('Borda Count', borda_count_voting['knn'][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "types_sample = [\n",
    "    'cnn',\n",
    "    'rf',\n",
    "    'knn'\n",
    "]\n",
    "\n",
    "for client in client_list:\n",
    "    print('Client: ', client.name)\n",
    "    \n",
    "    for type_sample in types_sample:\n",
    "        print(client.models[type_sample])\n",
    "        \n",
    "        if type_sample != 'cnn':\n",
    "            print(\n",
    "                'Score model: {:.2%}'.format(\n",
    "                    client.models[type_sample].score(\n",
    "                        dataset.get_test_images(type_sample),\n",
    "                        dataset.get_test_labels(type_sample)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                'Score model: {:.2%}'.format(\n",
    "                    client.models[type_sample].evaluate(\n",
    "                        dataset.get_test_images(type_sample),\n",
    "                        dataset.get_test_labels(type_sample),\n",
    "                        verbose=0\n",
    "                    )[1]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        \n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LABELS_TEST = KNN_LABELS\n",
    "\n",
    "\n",
    "def normalize_decisions_of_models(ballot, models=['cnn', 'rf', 'knn']):\n",
    "    result = {}\n",
    "    \n",
    "    for model in models:        \n",
    "        for label, voting in enumerate(ballot[model]):\n",
    "            result.setdefault(label, []).append({\n",
    "                model: max(voting['voting'], key=voting['voting'].get),\n",
    "                'ballot': voting['voting']\n",
    "            })\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_models_decisions(decisions_normalized, models=['cnn', 'rf', 'knn']):\n",
    "    \n",
    "    chosen_labels = []\n",
    "    \n",
    "    for index, results in decisions_normalized.items():\n",
    "        chosen_ones = {}\n",
    "\n",
    "        for result in results:\n",
    "            value = max(result['ballot'], key=result['ballot'].get)\n",
    "            chosen_ones.setdefault(value, 0)\n",
    "            chosen_ones[value] += 1\n",
    "\n",
    "        if len(chosen_ones) == len(models):\n",
    "            undecideds = {}\n",
    "\n",
    "            for result in results:\n",
    "                value = max(result['ballot'], key=result['ballot'].get)\n",
    "                qtd_votes = result['ballot'][value]\n",
    "                undecideds[value] = qtd_votes\n",
    "\n",
    "            chosen_ones = undecideds\n",
    "        \n",
    "        chosen_labels.append(max(chosen_ones, key=chosen_ones.get))\n",
    "    \n",
    "    return chosen_labels\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "hits = 0\n",
    "print('\\033[1mMAJORITY VOTING\\033[0m\\n')\n",
    "for i, chosen in enumerate(get_models_decisions(normalize_decisions_of_models(ballot=majority_voting))):\n",
    "    print(\n",
    "        'Label: ', i,\n",
    "        'Expected: ', LABELS_TEST[i],\n",
    "        'Result: ', chosen,\n",
    "        'Status: ', ('Correct' if chosen == LABELS_TEST[i] else 'Incorrect')\n",
    "    )\n",
    "    \n",
    "    hits += 1 if chosen == LABELS_TEST[i] else 0\n",
    "    \n",
    "print('Hit rate {:.4%}'.format((hits / len(LABELS_TEST))))\n",
    "\n",
    "\n",
    "hits = 0\n",
    "print('\\n\\n\\033[1mWEIGHTED MAJORITY VOTING\\033[0m\\n')\n",
    "for i, chosen in enumerate(get_models_decisions(normalize_decisions_of_models(ballot=weighted_majority_voting))):\n",
    "    print(\n",
    "        'Label: ', i,\n",
    "        'Expected: ', LABELS_TEST[i],\n",
    "        'Result: ', chosen,\n",
    "        'Status: ', ('Correct' if chosen == LABELS_TEST[i] else 'Incorrect')\n",
    "    )\n",
    "    \n",
    "    hits += 1 if chosen == LABELS_TEST[i] else 0\n",
    "    \n",
    "print('Hit rate {:.4%}'.format((hits / len(LABELS_TEST))))\n",
    "\n",
    "\n",
    "hits = 0\n",
    "print('\\n\\n\\033[1mBORDA COUNT VOTING\\033[0m\\n')\n",
    "for i, chosen in enumerate(get_models_decisions(normalize_decisions_of_models(ballot=borda_count_voting))):\n",
    "    print(\n",
    "        'Label: ', i,\n",
    "        'Expected: ', LABELS_TEST[i],\n",
    "        'Result: ', chosen,\n",
    "        'Status: ', ('Correct' if chosen == LABELS_TEST[i] else 'Incorrect')\n",
    "    )\n",
    "    \n",
    "    hits += 1 if chosen == LABELS_TEST[i] else 0\n",
    "    \n",
    "print('Hit rate {:.4%}'.format((hits / len(LABELS_TEST))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
